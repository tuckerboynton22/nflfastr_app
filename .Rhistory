# because my computer is slow
# if you're trying this for a different problem, expand the range here
# by using more negative values
dials::learn_rate(range = c(-2, -0.5), trans = scales::log10_trans()),
dials::loss_reduction(),
sample_size = dials::sample_prop(),
size = grid_size
) %>%
dplyr::mutate(
# has to be between 0 and 1 for xgb
# for some reason mtry gives the number of columns rather than proportion
mtry = mtry / length(train_data),
# see note below
monotone_constraints = "(0, -1, 0, -1, -1, -1, 1, 1, 1, 1, -1)"
# for the monotone constraints
# these are notes to myself to make sure the constraints are in the right order
# the order of the constraints needs to match up with the columns in the df
# age_sq (?)
# oline_rank (+)
# new_team (?)
# ecr (-)
# depth_team (-)
# rookie (-)
# first_round_bias (+)
# lag_xpts (+)
# lag_games (+)
# lag_opp_share (+)
# preseason_sos (+)
) %>%
# make these the right names for xgb
dplyr::rename(
eta = learn_rate,
gamma = loss_reduction,
subsample = sample_size,
colsample_bytree = mtry,
max_depth = tree_depth,
min_child_weight = min_n
)
# function to perform xgb.cv for a given row in a hyperparameter grid
get_row <- function(row) {
params <-
list(
booster = "gbtree",
objective = "binary:logistic",
eval_metric = c("logloss"),
eta = row$eta,
gamma = row$gamma,
subsample = row$subsample,
colsample_bytree = row$colsample_bytree,
max_depth = row$max_depth,
min_child_weight = row$min_child_weight,
monotone_constraints = row$monotone_constraints,
tree_method = "hist"
)
# do the cross validation
rb_cv_model <- xgboost::xgb.cv(
data = as.matrix(train_data),
label = train_labels$label,
params = params,
# this doesn't matter with early stopping in xgb.cv, just set a big number
# the actual optimal rounds will be found in this tuning process
nrounds = 15000,
# created above
folds = folds,
metrics = list("logloss"),
early_stopping_rounds = 100,
print_every_n = 50
)
# bundle up the results together for returning
output <- params
output$iter <- rb_cv_model$best_iteration
output$logloss <- rb_cv_model$evaluation_log[output$iter]$test_logloss_mean
row_result <- bind_rows(output)
return(row_result)
}
results <- purrr::map_df(1:nrow(grid), function(x) {
get_row(grid %>% dplyr::slice(x))
})
best_model <- results %>%
dplyr::arrange(logloss) %>%
dplyr::slice(1)
params <-
list(
booster = "gbtree",
objective = "binary:logistic",
eval_metric = c("logloss"),
eta = best_model$eta,
gamma = best_model$gamma,
subsample = best_model$subsample,
colsample_bytree = best_model$colsample_bytree,
max_depth = best_model$max_depth,
min_child_weight = best_model$min_child_weight,
monotone_constraints = best_model$monotone_constraints,
tree_method = "hist"
)
nrounds <- best_model$iter
rb_model <- xgboost::xgboost(
params = params,
data = as.matrix(train_data),
label = train_labels$label,
nrounds = nrounds,
verbose = 2
)
importance <- xgboost::xgb.importance(
feature_names = colnames(rb_model),
model = rb_model
)
xgboost::xgb.ggplot.importance(importance_matrix = importance)
preds <- stats::predict(
rb_model,
# get rid of the things not needed for prediction here
as.matrix(test_data %>% select(-all_of(nonmodel_features), -label))
) %>%
tibble::as_tibble() %>%
dplyr::rename(p_top_12 = value) %>%
dplyr::bind_cols(test_data) %>%
group_by(season) %>%
mutate(
my_preseason_rank = rank(-p_top_12, ties.method = "min"),
rank_diff = my_preseason_rank - preseason_rank
) %>%
select(full_name, season, preseason_rank, my_preseason_rank, postseason_rank)
cor(preds$my_preseason_rank, preds$postseason_rank)
cor(preds$preseason_rank, preds$postseason_rank)
View(preds)
preds <- stats::predict(
rb_model,
# get rid of the things not needed for prediction here
as.matrix(test_data %>% select(-all_of(nonmodel_features), -label))
) %>%
tibble::as_tibble() %>%
dplyr::rename(p_top_12 = value) %>%
dplyr::bind_cols(test_data) %>%
group_by(season) %>%
mutate(
my_preseason_rank = rank(-p_top_12, ties.method = "min"),
rank_diff = my_preseason_rank - preseason_rank
)
rb_improvement <- player_stats %>%
filter(depth_team == 1, position == "WR", !is.na(roster_pos_change)) %>%
mutate(improvement = points - lag_xpts) %>%
select(full_name, season, posteam, points, lag_xpts, lag_opp_share, roster_pos_change, improvement)
rb_improvement %>%
ggplot(aes(roster_pos_change, improvement)) +
statar::stat_binmean()
cor(rb_improvement$improvement, rb_improvement$roster_pos_change)
View(player_stats)
## COMBINE IT ALL INTO PLAYER STATS DF
player_stats <- opportunity %>%
filter(position == "QB" | position == "RB" | position == "TE" | position == "WR") %>%
group_by(player_id, season) %>%
summarize(
xpoints = sum(xpoints, na.rm = T),
points = sum(points, na.rm = T),
full_name = first(full_name),
posteam = first(posteam)
) %>%
left_join(rosters, by=c("player_id"="gsis_id")) %>%
filter(!is.na(fantasypros_id)) %>%
mutate(season = as.numeric(season),
age = lubridate::time_length(difftime(paste0(season, "-09-01"), birthdate), "years")) %>%
left_join(depth_charts, by=c("player_id"="gsis_id", "season")) %>%
left_join(ecr_pre, by=c("fantasypros_id", "season"="year")) %>%
ungroup() %>%
mutate(rookie = ifelse(draft_year == season, 1, 0),
first_rounder = ifelse(draft_round == 1, 1, 0),
first_round_bias = ifelse(season - draft_year > 4, 0, (draft_year - season + 4)*first_rounder)) %>%
select(-c(draft_year, draft_round)) %>%
arrange(season) %>%
group_by(player_id) %>%
mutate(lag_pts = lag(points),
lag_pts = ifelse(is.na(lag_pts), 0, lag_pts),
lag_xpts = lag(xpoints),
lag_xpts = ifelse(is.na(lag_xpts), 0, lag_xpts),
lag_tm = lag(posteam),
new_team = ifelse(posteam == lag_tm, 0, 1),
new_team = ifelse(is.na(new_team), 1, 0),
lag_season = lag(season)) %>%
ungroup() %>%
filter(season >= 2012) %>%
left_join(pos_offenses, by=c("lag_season"="season", "lag_tm"="posteam", "position")) %>%
rename(lag_team_pos_xpts = total_fantasy_points_exp) %>%
left_join(pos_offenses, by=c("lag_season"="season", "posteam", "position")) %>%
rename(team_pos_lag_xpts = total_fantasy_points_exp) %>%
left_join(preseason_pos_roster_strength, by=c("season", "posteam"="team", "position")) %>%
mutate(
roster_pos_change = roster_lag_xpoints - lag_team_pos_xpts,
roster_pos_change = ifelse(is.na(roster_pos_change), 0, roster_pos_change),
situation_improvement = team_pos_lag_xpts - lag_team_pos_xpts,
situation_improvement = ifelse(is.na(situation_improvement), 0, situation_improvement),
first_round_bias = ifelse(is.na(first_round_bias), 0, first_round_bias),
lag_opp_share = lag_xpts/lag_team_pos_xpts,
lag_opp_share = ifelse(is.na(lag_opp_share), 0, lag_opp_share),
id = paste0(player_id, "_", season),
ecr_low = ecr - sd,
ecr_low = ifelse(ecr_low < 0, 0, ecr_low),
ecr_high = ecr + sd,
efficiency = lag_pts/lag_xpts,
age_sq = age**2
) %>%
filter(!is.na(ecr)) %>%
left_join(sos, by = c("posteam"="team", "season", "position")) %>%
left_join(games_played, by = c("lag_season"="season", "player_id")) %>%
mutate(lag_games = ifelse(is.na(lag_games), 0, lag_games)) %>%
group_by(season, position) %>%
mutate(postseason_rank = rank(-points, ties.method = "min"),
preseason_rank = rank(ecr, ties.method = "min")) %>%
ungroup() %>%
left_join(oline_ranks, by = c("posteam"="team", "season")) %>%
mutate(depth_team = ifelse(is.na(depth_team), 4, depth_team),
depth_team = as.numeric(depth_team),
top_12 = ifelse(postseason_rank < 13, 1, 0)) %>%
select(full_name, position, id, season, posteam, points, preseason_rank, postseason_rank, top_12, age_sq, oline_rank,
new_team, ecr, depth_team, rookie, first_round_bias, lag_xpts, lag_games, lag_opp_share, preseason_sos, roster_pos_change) %>%
filter(!is.na(preseason_sos))
player_stats %>%
select(points:roster_pos_change) %>%
cor() %>%
View()
player_stats %>%
filter(position == "RB", team_depth < 3) %>%
select(points:roster_pos_change) %>%
cor() %>%
View()
player_stats %>%
filter(position == "RB", depth_team < 3) %>%
select(points:roster_pos_change) %>%
cor() %>%
View()
# options(remove(list = ls()))
grid_size <- 40
nonmodel_features <-
c("full_name",
"position",
"id",
"season",
"posteam",
"points",
"postseason_rank",
"preseason_rank")
xgboost_df <- player_stats %>%
filter(position == "RB", depth_team < 3) %>%
rename(label = top_12)
xgboost_df <- player_stats %>%
filter(position == "RB", depth_team < 3) %>%
rename(label = points)
nonmodel_features <-
c("full_name",
"position",
"id",
"season",
"posteam",
"top_12",
"postseason_rank",
"preseason_rank")
xgboost_df <- player_stats %>%
filter(position == "RB", depth_team < 3) %>%
rename(label = points)
test_data <- xgboost_df %>%
dplyr::filter(season >= 2020)
train_data <- xgboost_df %>%
dplyr::filter(season < 2020)
# explanation of this step below
folds <- splitTools::create_folds(
y = train_data$id,
k = 5,
type = "basic",
invert = TRUE
)
train_labels <- train_data %>%
dplyr::select(label)
# get rid of extra columns
train_data <- train_data %>%
dplyr::select(-all_of(nonmodel_features), -label)
View(train_data)
grid <- dials::grid_latin_hypercube(
# this finalize thing is because mtry depends on # of columns in data
dials::finalize(dials::mtry(), train_data),
dials::min_n(),
dials::tree_depth(),
# to force learn_rate to not be crazy small like dials defaults to
# because my computer is slow
# if you're trying this for a different problem, expand the range here
# by using more negative values
dials::learn_rate(range = c(-2, -0.5), trans = scales::log10_trans()),
dials::loss_reduction(),
sample_size = dials::sample_prop(),
size = grid_size
) %>%
dplyr::mutate(
# has to be between 0 and 1 for xgb
# for some reason mtry gives the number of columns rather than proportion
mtry = mtry / length(train_data),
# see note below
monotone_constraints = "(0, -1, 0, -1, -1, -1, 1, 1, 1, 1, -1)"
# for the monotone constraints
# these are notes to myself to make sure the constraints are in the right order
# the order of the constraints needs to match up with the columns in the df
# age_sq (?)
# oline_rank (+)
# new_team (?)
# ecr (-)
# depth_team (-)
# rookie (-)
# first_round_bias (+)
# lag_xpts (+)
# lag_games (+)
# lag_opp_share (+)
# preseason_sos (+)
) %>%
# make these the right names for xgb
dplyr::rename(
eta = learn_rate,
gamma = loss_reduction,
subsample = sample_size,
colsample_bytree = mtry,
max_depth = tree_depth,
min_child_weight = min_n
)
# function to perform xgb.cv for a given row in a hyperparameter grid
get_row <- function(row) {
params <-
list(
booster = "gblinear",
objective = "reg:squarederror",
eval_metric = c("rmse"),
eta = row$eta,
gamma = row$gamma,
subsample = row$subsample,
colsample_bytree = row$colsample_bytree,
max_depth = row$max_depth,
min_child_weight = row$min_child_weight,
monotone_constraints = row$monotone_constraints,
tree_method = "auto"
)
# do the cross validation
rb_cv_model <- xgboost::xgb.cv(
data = as.matrix(train_data),
label = train_labels$label,
params = params,
# this doesn't matter with early stopping in xgb.cv, just set a big number
# the actual optimal rounds will be found in this tuning process
nrounds = 15000,
# created above
folds = folds,
metrics = list("mae"),
early_stopping_rounds = 100,
print_every_n = 50
)
# bundle up the results together for returning
output <- params
output$iter <- rb_cv_model$best_iteration
output$logloss <- rb_cv_model$evaluation_log[output$iter]$test_logloss_mean
row_result <- bind_rows(output)
return(row_result)
}
results <- purrr::map_df(1:nrow(grid), function(x) {
get_row(grid %>% dplyr::slice(x))
})
best_model <- results %>%
dplyr::arrange(logloss) %>%
dplyr::slice(1)
params <-
list(
booster = "gbtree",
objective = "binary:logistic",
eval_metric = c("logloss"),
eta = best_model$eta,
gamma = best_model$gamma,
subsample = best_model$subsample,
colsample_bytree = best_model$colsample_bytree,
max_depth = best_model$max_depth,
min_child_weight = best_model$min_child_weight,
monotone_constraints = best_model$monotone_constraints,
tree_method = "hist"
)
nrounds <- best_model$iter
rb_model <- xgboost::xgboost(
params = params,
data = as.matrix(train_data),
label = train_labels$label,
nrounds = nrounds,
verbose = 2
)
# function to perform xgb.cv for a given row in a hyperparameter grid
get_row <- function(row) {
params <-
list(
booster = "dart",
objective = "reg:squarederror",
eval_metric = c("rmse"),
eta = row$eta,
gamma = row$gamma,
subsample = row$subsample,
colsample_bytree = row$colsample_bytree,
max_depth = row$max_depth,
min_child_weight = row$min_child_weight,
monotone_constraints = row$monotone_constraints,
tree_method = "auto"
)
# do the cross validation
rb_cv_model <- xgboost::xgb.cv(
data = as.matrix(train_data),
label = train_labels$label,
params = params,
# this doesn't matter with early stopping in xgb.cv, just set a big number
# the actual optimal rounds will be found in this tuning process
nrounds = 15000,
# created above
folds = folds,
metrics = list("mae"),
early_stopping_rounds = 100,
print_every_n = 50
)
# bundle up the results together for returning
output <- params
output$iter <- rb_cv_model$best_iteration
output$logloss <- rb_cv_model$evaluation_log[output$iter]$test_logloss_mean
row_result <- bind_rows(output)
return(row_result)
}
results <- purrr::map_df(1:nrow(grid), function(x) {
get_row(grid %>% dplyr::slice(x))
})
# function to perform xgb.cv for a given row in a hyperparameter grid
get_row <- function(row) {
params <-
list(
booster = "gblinear",
objective = "reg:squarederror",
eval_metric = c("rmse"),
eta = row$eta,
gamma = row$gamma,
subsample = row$subsample,
colsample_bytree = row$colsample_bytree,
max_depth = row$max_depth,
min_child_weight = row$min_child_weight,
monotone_constraints = row$monotone_constraints,
tree_method = "hist"
)
# do the cross validation
rb_cv_model <- xgboost::xgb.cv(
data = as.matrix(train_data),
label = train_labels$label,
params = params,
# this doesn't matter with early stopping in xgb.cv, just set a big number
# the actual optimal rounds will be found in this tuning process
nrounds = 15000,
# created above
folds = folds,
metrics = list("mae"),
early_stopping_rounds = 100,
print_every_n = 50
)
# bundle up the results together for returning
output <- params
output$iter <- rb_cv_model$best_iteration
output$logloss <- rb_cv_model$evaluation_log[output$iter]$test_logloss_mean
row_result <- bind_rows(output)
return(row_result)
}
results <- purrr::map_df(1:nrow(grid), function(x) {
get_row(grid %>% dplyr::slice(x))
})
best_model <- results %>%
dplyr::arrange(logloss) %>%
dplyr::slice(1)
params <-
list(
booster = "gbtree",
objective = "binary:logistic",
eval_metric = c("logloss"),
eta = best_model$eta,
gamma = best_model$gamma,
subsample = best_model$subsample,
colsample_bytree = best_model$colsample_bytree,
max_depth = best_model$max_depth,
min_child_weight = best_model$min_child_weight,
monotone_constraints = best_model$monotone_constraints,
tree_method = "hist"
)
View(results)
best_model <- results %>%
# dplyr::arrange(logloss) %>%
dplyr::slice(1)
params <-
list(
booster = "gbtree",
objective = "binary:logistic",
eval_metric = c("logloss"),
eta = best_model$eta,
gamma = best_model$gamma,
subsample = best_model$subsample,
colsample_bytree = best_model$colsample_bytree,
max_depth = best_model$max_depth,
min_child_weight = best_model$min_child_weight,
monotone_constraints = best_model$monotone_constraints,
tree_method = "hist"
)
nrounds <- best_model$iter
rb_model <- xgboost::xgboost(
params = params,
data = as.matrix(train_data),
label = train_labels$label,
nrounds = nrounds,
verbose = 2
)
importance <- xgboost::xgb.importance(
feature_names = colnames(rb_model),
model = rb_model
)
xgboost::xgb.ggplot.importance(importance_matrix = importance)
